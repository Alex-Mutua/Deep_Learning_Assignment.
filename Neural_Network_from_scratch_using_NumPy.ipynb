{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEG1dhnuNFRk"
      },
      "source": [
        "# Steps to build a Neural Network in NumPy\n",
        "\n",
        "\n",
        "\n",
        "## Assignment Overview\n",
        "In this assignment, you will be developing a basic neural network from scratch using Python. The goal is to understand the fundamental components of a neural network by implementing the crucial processes that drive its learning capabilities.\n",
        "\n",
        "You will fill in the `.......` sections with the necessary code that corresponds to the outlined tasks. Each section requires you to consider mathematical formulations and dimensions of the variables used, which will help reinforce your understanding of how data flows through the network and how various components interact.\n",
        "\n",
        "**Important Sections:**\n",
        "\n",
        "1. **Loading the Dataset (Input and Output)**  \n",
        "   This section involves reading and preprocessing a dataset that the neural network will learn from. You will load features (inputs) and labels (outputs) that the model will try to predict.\n",
        "\n",
        "2. **Architecture of the Model (# Input, Hidden and Output Neurons)**  \n",
        "   Here, you will define the structure of the neural network. It is essential to determine the number of neurons in the input layer (which corresponds to the number of features in your dataset), the hidden layer(s) (which will be responsible for learning complex representations), and the output layer (which will provide the final predictions).\n",
        "\n",
        "3. **Initializing Weights for All Layers**  \n",
        "   In this section, you will set up the initial weights and biases for the neurons. Proper initialization is crucial for ensuring that the learning process starts effectively, avoiding issues such as vanishing or exploding gradients.\n",
        "\n",
        "4. **Implementing Forward Propagation**  \n",
        "   Forward propagation is where you compute the output of the network given its inputs and current weights. By applying activation functions, you will pass the inputs through the network, layer by layer, to obtain the final prediction.\n",
        "\n",
        "5. **Implementing Backward Propagation**  \n",
        "   Backward propagation is the process of updating the weights based on the error between the predicted output and the actual labels. You will compute gradients of the loss function with respect to the weights and use these to make adjustments that minimize the error.\n",
        "\n",
        "6. **Train the Model for n Epochs**  \n",
        "   This section will involve iterating over the training data for a specified number of epochs, applying forward and backward propagation to refine the model's weights. During training, you will monitor the loss to evaluate the model's performance and convergence.\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions for Completing the Assignment\n",
        "\n",
        "For each of the sections mentioned above, do the following:\n",
        "\n",
        "- Review the explanations provided.\n",
        "- Fill in the `.......` parts with code snippets that fulfill the requirements of that section.\n",
        "- Before each code snippet for which you are filling in the `.......` parts, provide the **Mathematical notation** (after `Mathematical notation:`), which represents the underlying equations used in that part of the process, and the **Dimensions** (after `Dimensions:`) to indicate the shape of the variables involved (i.e., how many samples, features, etc.).\n",
        "\n",
        "By the end of this assignment, you will actively engage with key aspects of neural networks, linking theoretical concepts with their practical implementation. This hands-on experience will be invaluable for solidifying your understanding of machine learning fundamentals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgj7HfHNFRr"
      },
      "source": [
        "## 1. Loading the dataset (Input and Output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5S9HgBzNFRw"
      },
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogs6CaXu2zeZ",
        "outputId": "79bc87a6-7954-4ca3-cbe3-75f6f48181c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "# version of numpy library\n",
        "print('Version of numpy:',np.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version of numpy: 1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNmvxGv723N6",
        "outputId": "74eb1897-cdd9-45c1-e7e7-435e86f23014",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# version of matplotlib library\n",
        "print('Version of matplotlib:',matplotlib.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version of matplotlib: 3.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_h7HoPONFR_",
        "outputId": "0b49c7c0-b22c-44e6-9a9d-db6b7c62ce13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating the input array\n",
        "X=np.array([[1,0,1,0,1], [1,0,1,1,0], [1,1,0,1,1],[1,0,1,0,1],[1,1,1,0,1],[1,1,1,1,1],[1,0,0,0,1],[0,0,0,0,1]])\n",
        "print ('\\n Input:')\n",
        "print(X)\n",
        "# shape of input array\n",
        "print('\\n Shape of Input:', X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input:\n",
            "[[1 0 1 0 1]\n",
            " [1 0 1 1 0]\n",
            " [1 1 0 1 1]\n",
            " [1 0 1 0 1]\n",
            " [1 1 1 0 1]\n",
            " [1 1 1 1 1]\n",
            " [1 0 0 0 1]\n",
            " [0 0 0 0 1]]\n",
            "\n",
            " Shape of Input: (8, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVvQz5g39wo3",
        "outputId": "73727a44-1597-4a1d-cc20-12523a89e30c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# converting the input in matrix form\n",
        "X = X.T\n",
        "print('\\n Input in matrix form:')\n",
        "print(X)\n",
        "# shape of input matrix\n",
        "print('\\n Shape of Input Matrix:', X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Input in matrix form:\n",
            "[[1 1 1 1 1 1 1 0]\n",
            " [0 0 1 0 1 1 0 0]\n",
            " [1 1 0 1 1 1 0 0]\n",
            " [0 1 1 0 0 1 0 0]\n",
            " [1 0 1 1 1 1 1 1]]\n",
            "\n",
            " Shape of Input Matrix: (5, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRe8JE0xNFSL",
        "outputId": "b1c31a07-9b70-4975-fa14-9bd141fa31de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating the output array\n",
        "y=np.array([[1],[1],[0],[1],[1],[0],[1],[1]])\n",
        "print ('\\n Actual Output:')\n",
        "print(y)\n",
        "\n",
        "# output in matrix form\n",
        "y = y.T\n",
        "\n",
        "print ('\\n Output in matrix form:')\n",
        "print(y)\n",
        "\n",
        "# shape of input array\n",
        "print('\\n Shape of Output:', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Actual Output:\n",
            "[[1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            " Output in matrix form:\n",
            "[[1 1 0 1 1 0 1 1]]\n",
            "\n",
            " Shape of Output: (1, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKf4Ji1-NFSV"
      },
      "source": [
        "## 2. Architecture of the model (# input, hidden and output neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhBW0NNNFSg"
      },
      "source": [
        "inputlayer_neurons = X.shape[0] # number of features in data set\n",
        "hiddenlayer_neurons = 3 # number of hidden layers neurons\n",
        "output_neurons = 1 # number of neurons at output layer\n",
        "# inputlayer_neurons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoOsLucmNFSo"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1zrEFVsc6bMQZ7fRxbK4DRceaG78k26Pc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nmwj8RfNFSr"
      },
      "source": [
        "## 3. Initializing the weights for all the layers\n",
        "\n",
        "NOTE: For simplicity, we are assuming that the bias for all the layers is 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T1IG-W8NFSu"
      },
      "source": [
        "# initializing weight\n",
        "# Shape of w_ih should number of neurons at input layer X number of neurons at hidden layer\n",
        "w_ih=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "\n",
        "\n",
        "# Shape of w_ho should number of neurons at hidden layer X number of neurons at output layer\n",
        "w_ho=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpa1--9KNFS1",
        "outputId": "00f8298b-44d5-4603-9897-f66e7c96a81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# shape of weight matrix\n",
        "w_ih.shape, w_ho.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5, 3), (3, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srrDW1MNNFS-"
      },
      "source": [
        "## 4. Implementing forward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g-SocwQNFTC"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1YwD7vY9k84vZmjmE5CXgQ69fYyadPsox)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcBji4iNFTE"
      },
      "source": [
        "# We are using sigmoid as an activation function so defining the sigmoid function here\n",
        "\n",
        "# defining the Sigmoid Function\n",
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO6AYHtGNFTM"
      },
      "source": [
        "# hidden layer activations\n",
        "\n",
        "hidden_layer_input=np.dot(w_ih.T,X)\n",
        "hiddenlayer_activations = sigmoid(hidden_layer_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_input.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WItS_DrJU8qe",
        "outputId": "49cbaa58-e4f2-427b-9150-903e333c4b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8zzYX6pNFTT"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1ETMoLD1fwi5u1HHLqtAdVUs-P8HNOU_p)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuqKwiToNFTW"
      },
      "source": [
        "# calculating the output\n",
        "output_layer_input=np.dot(w_ho.T,hiddenlayer_activations)\n",
        "output = sigmoid(output_layer_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz3O1IriehjK",
        "outputId": "a6b4d6df-754d-466c-d363-e8cae92bc217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjPlMkVMNFTd",
        "outputId": "9bdf8a4f-0fdf-4bba-e4ac-e11cf47bab33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# output\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.76621183, 0.76636302, 0.78543985, 0.76621183, 0.78580513,\n",
              "        0.79164897, 0.7452827 , 0.70317909]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdFKMYyzNFTm"
      },
      "source": [
        "## 5. Implementing backward propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2m3XBgZNFTn"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1uYdg4mQL-B9o7BTOLnfoYUhh_LxTnpcW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUAAhlcNFTp"
      },
      "source": [
        "# calculating error\n",
        "error = np.square(y-output)/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtUQEkINew_s",
        "outputId": "3a56ab29-bf77-414f-e012-39d5b0e29803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02732845, 0.02729312, 0.30845788, 0.02732845, 0.02293972,\n",
              "        0.31335405, 0.03244045, 0.04405133]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H0vjBdNNFTw"
      },
      "source": [
        "### Rate of change of error w.r.t weight between hidden and output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cncCd1WNFTz"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1_KexjgVJGRptZ6t1eobTter3mfIGo9rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqrhlDeDNFT1"
      },
      "source": [
        "**a. Rate of change of error w.r.t output**\n",
        "\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find $\\frac{dE}{dO}$, we start with the error function defined as $E = \\frac{1}{2}(Y - O)^2$. This function measures how far off the output $O$ is from the target value $Y$.\n",
        "\n",
        "\n",
        "Next, we differentiate $E$ with respect to $O$ to see how the error changes as the output changes. The differentiation process yields $\\frac{dE}{dO} = \\frac{d}{dO}\\left(\\frac{1}{2}(Y - O)^2\\right)$.\n",
        "\n",
        "\n",
        "To do this, we use the chain rule. The outer part of the function is $\\frac{1}{2}(u^2)$, where $u = Y - O$. The derivative of this outer function with respect to $u$ is $u$, and the inner function $u$ has a derivative of $-1$ since we are differentiating $-O$.\n",
        "\n",
        "\n",
        "Putting it all together, we find that $\\frac{dE}{dO} = (Y - O) \\cdot (-1)$, which simplifies to $\\frac{dE}{dO} = -(Y - O)$.\n",
        "\n",
        "\n",
        "This result indicates that as the output $O$ approaches the target $Y$, the error $E$ decreases, which helps guide adjustments in the model during training.\n",
        "\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To find the dimensions involved in this derivative, we note that the inputs $X$ and the targets $y$ are given in transposed form. Specifically, $X$ has dimensions $(m, n)$ (where $m$ is the number of features and $n$ is the number of observations), and the target $y$ is represented as $(1, n)$. Consequently, the output $O$ from the model also has dimensions $(1, n)$.\n",
        "\n",
        "\n",
        "Thus, the dimension of the error $E$ calculated as $E = \\frac{1}{2}(y - O)^2$ remains $(1, n)$, as both $y$ and $O$ have the same dimensions.\n",
        "\n",
        "\n",
        "To find the dimension of the partial derivative $\\frac{dE}{dO}$, we compare the dimensions: the dimension of $E$ is $(1, n)$ and the dimension of $O$ is also $(1, n)$.\n",
        "\n",
        "\n",
        "Therefore, we conclude that the dimension of $\\frac{dE}{dO}$ is:\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dE}{dO} = \\frac{(1, n)}{(1, n)} = (1, n).\n",
        "$$\n",
        "\n",
        "\n",
        "This means that the partial derivative $\\frac{dE}{dO}$ has 1 row and $n$ columns, indicating that it corresponds to the single output neuron for each of the $n$ observations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKdk5m4FNFT3"
      },
      "source": [
        "# rate of change of error w.r.t. output\n",
        "error_wrt_output = -(y-output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Rate of change of output w.r.t Z2**\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find $\\frac{dO}{dz_2}$, we start with the sigmoid activation function defined as $O = \\sigma(z_2) = \\frac{1}{1 + e^{-z_2}}$. This function maps the input $z_2$ to an output $O$ between 0 and 1.\n",
        "\n",
        "\n",
        "Next, we differentiate $O$ with respect to $z_2$ to see how the output changes as the input $z_2$ changes. The differentiation process yields $\\frac{dO}{dz_2} = \\frac{d}{dz_2}\\left(\\frac{1}{1 + e^{-z_2}}\\right)$.\n",
        "\n",
        "\n",
        "To do this, we can apply the chain rule. The outer part of the function is $(1 + e^{-z_2})^{-1}$. The derivative of this outer function with respect to the inner function $u = 1 + e^{-z_2}$ is $-\\frac{1}{u^2}$. The inner function $u$ has a derivative of $e^{-z_2}(-1) = -e^{-z_2}$ since we are differentiating $e^{-z_2}$.\n",
        "\n",
        "\n",
        "Putting it all together, we find that:\n",
        "$$\n",
        "\\frac{dO}{dz_2} = -\\frac{1}{(1 + e^{-z_2})^2} \\cdot (-e^{-z_2}) = \\frac{e^{-z_2}}{(1 + e^{-z_2})^2}\n",
        "$$\n",
        "\n",
        "\n",
        "Now, we can express this result in terms of $O$. Since $O = \\frac{1}{1 + e^{-z_2}}$, we use the identity $e^{-z_2} = \\frac{1 - O}{O}$ to rewrite the derivative:\n",
        "$$\n",
        "\\frac{dO}{dz_2} = O(1 - O)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To find the dimensions involved in the derivative $\\frac{dO}{dz_2}$, we start with the input $z_2$ that is calculated from the hidden layer outputs $h_1$ and weights $W_{ho}$ as follows:\n",
        "\n",
        "\n",
        "$$\n",
        "z_2 = W_{ho} h_1 + b_{ho}\n",
        "$$\n",
        "\n",
        "\n",
        "In this equation, $W_{ho}$ connects the hidden layer to the output layer. The weight matrix $W_{ho}$ has dimensions $(1, m_1)$ (where $m_1$ is the number of neurons in the hidden layer), while the output from the hidden layer $h_1$ has dimensions $(m_1, n)$ (where $n$ is the number of observations). Additionally, the bias $b_{ho}$ has dimensions $(1, n)$.\n",
        "\n",
        "\n",
        "Therefore, the dimension of $z_2$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } z_2 = (1, n)\n",
        "$$\n",
        "\n",
        "\n",
        "Next, we use the sigmoid function to compute the output $O$:\n",
        "\n",
        "\n",
        "$$\n",
        "O = \\sigma(z_2)\n",
        "$$\n",
        "\n",
        "\n",
        "Since the output $O$ is generated for each element in $z_2$ through the sigmoid function, we find that the dimension of $O$ is also:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } O = (1, n)\n",
        "$$\n",
        "\n",
        "\n",
        "To find the dimension of the derivative $\\frac{dO}{dz_2}$, we compare the dimensions involved. The dimension of $O$ is $(1, n)$ and the dimension of $z_2$ is also $(1, n)$.\n",
        "\n",
        "\n",
        "Thus, we conclude that the dimension of $\\frac{dO}{dz_2}$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dO}{dz_2} = \\frac{(1, n)}{(1, n)} = (1, n).\n",
        "$$\n",
        "\n",
        "\n",
        "This indicates that the partial derivative $\\frac{dO}{dz_2}$ has 1 row and $n$ columns, corresponding to the single output neuron for each of the $n$ observations.\n"
      ],
      "metadata": {
        "id": "xNPrl-_RlRLs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl1PDwrBNFT9"
      },
      "source": [
        "# rate of change of output w.r.t. Z2\n",
        "output_wrt_Z2 = np.multiply(output,(1-output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Rate of change of Z2 w.r.t weights between hidden and output layer**\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find $\\frac{dZ_2}{dW_{ho}}$, we start by defining the values involved in the computation of $Z_2$. The total input to the second layer is given by:\n",
        "\n",
        "\n",
        "$$\n",
        "Z_2 = W_{ho}^T h_1 + b_{ho}\n",
        "$$\n",
        "\n",
        "\n",
        "where $h_1 = \\sigma(Z_1)$ is the output from the first layer, and $\\sigma$ is the activation function.\n",
        "\n",
        "\n",
        "Next, we differentiate $Z_2$ with respect to $W_{ho}$:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_2}{dW_{ho}} = \\frac{d}{dW_{ho}}(W_{ho}^T h_1 + b_{ho})\n",
        "$$\n",
        "\n",
        "\n",
        "Since $b_{ho}$ does not depend on $W_{ho}$, we can ignore it while taking the derivative. Using the product rule on the term $W_{ho}^T h_1$, we get:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_2}{dW_{ho}} = h_1\n",
        "$$\n",
        "\n",
        "\n",
        "This indicates that the change in $Z_2$ with respect to $W_{ho}$ is dependent on the values of $h_1$ from the first layer. Since $h_1$ is computed using the activation function applied to $Z_1$, this value will influence how $Z_2$ changes as we adjust $W_{ho}$.\n",
        "\n",
        "\n",
        "In summary, we find that:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_2}{dW_{ho}} = h_1\n",
        "$$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dZ_2}{dW_{ho}}$ is equal to the output of the first layer, which will guide the adjustment of the weights $W_{ho}$ during the training process.\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To find the dimensions involved in the derivative $\\frac{dZ_2}{dW_{ho}}$, we first note the relevant dimensions of the components. The matrix $W_{ho}$ has dimensions $(m_1, 1)$, where $m_1$ is the number of neurons in the second layer. The output $h_1$ from the first layer has dimensions $(m_1, n)$, where $n$ is the number of observations.\n",
        "\n",
        "\n",
        "Now we examine the expression for $Z_2$:\n",
        "\n",
        "\n",
        "$$\n",
        "Z_2 = W_{ho}^T h_1 + b_{ho}\n",
        "$$\n",
        "\n",
        "\n",
        "Here, $b_{ho}$ is a bias term which usually has dimensions $(1, n)$ (broadcasting across observations). We focus on the leading term $W_{ho}^T h_1$.\n",
        "\n",
        "\n",
        "The dimensions for the matrix multiplication $W_{ho}^T h_1$ are calculated as follows. $W_{ho}^T$ has dimensions $(1, m_1)$ and $h_1$ has dimensions $(m_1, n)$. Thus, the product $W_{ho}^T h_1$ results in dimensions\n",
        "\n",
        "\n",
        "$$\n",
        "(1, m_1) \\times (m_1, n) = (1, n).\n",
        "$$\n",
        "\n",
        "\n",
        "Now, considering the derivative $\\frac{dZ_2}{dW_{ho}}$, we have established that $\\frac{dZ_2}{dW_{ho}} = h_1$, and since $h_1$ has dimensions $(m_1, n)$, we conclude that the dimension of $\\frac{dZ_2}{dW_{ho}}$ is\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dZ_2}{dW_{ho}} = (m_1, n).\n",
        "$$\n",
        "\n",
        "\n",
        "This indicates that for each of the $n$ observations, there are $m_1$ partial derivatives corresponding to the $m_1$ neurons in the second layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "vEbrg-4OlTCW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vLk1nxLNFUD"
      },
      "source": [
        "# rate of change of Z2 w.r.t. weights between hidden and output layer\n",
        "Z2_wrt_who = hiddenlayer_activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXXifY9QNFUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bc999c-b191-483b-a4f0-2bfef3753192"
      },
      "source": [
        "# checking the shapes of partial derivatives\n",
        "error_wrt_output.shape, output_wrt_Z2.shape, Z2_wrt_who.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 8), (1, 8), (3, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvtS7wCRNFUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f8f26e-0dbc-4bf4-9153-fae627c7b446"
      },
      "source": [
        "# shape of weights of output layer\n",
        "w_ho.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9zQEH6HON5"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1VesmZOVpfgLFESvOFd7dE-YHNtSMMkvM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3HNVYGONFUr"
      },
      "source": [
        "# rate of change of error w.r.t weight between hidden and output layer\n",
        "error_wrt_who = np.dot(Z2_wrt_who,(error_wrt_output*output_wrt_Z2).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwyI1EGZNFUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdc87db-5d4b-4647-e722-d40de6e995ec"
      },
      "source": [
        "error_wrt_who.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDFPg2SHNFU2"
      },
      "source": [
        "### Rate of change of error w.r.t weight between input and hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_757MrjBNFU2"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1X4-iInwlv7ber3fwgtqHuHTuFRci-tMV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nPYGXkeNFU4"
      },
      "source": [
        "**a. Rate of change of error w.r.t output**\n",
        "\n",
        "- `Mathematical notation:` See part a) of \"Rate of change of error w.r.t weight between hidden and output layer\"\n",
        "- `Dimensions:` See part a) of \"Rate of change of error w.r.t weight between hidden and output layer\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb7Ezxw9NFU6"
      },
      "source": [
        "# rate of change of error w.r.t. output\n",
        "error_wrt_output = -(y-output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Rate of change of output w.r.t Z2**\n",
        "- `Mathematical notation:` See part b) of \"Rate of change of error w.r.t weight between hidden and output layer\"\n",
        "- `Dimensions:` See part b) of \"Rate of change of error w.r.t weight between hidden and output layer\""
      ],
      "metadata": {
        "id": "Pl2Vfu33nbKp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-SGbNaoNFVA"
      },
      "source": [
        "# rate of change of output w.r.t. Z2\n",
        "output_wrt_Z2 = np.multiply(output,(1-output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Rate of change of Z2 w.r.t hidden layer activations**\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find $\\frac{dZ_2}{dh_1}$, we start with the equation for $Z_2$, which is given by:\n",
        "\n",
        "\n",
        "$\n",
        "Z_2 = W_{ho}^T h_1 + b_{ho}\n",
        "$\n",
        "\n",
        "\n",
        "Here, $W_{ho}$ is the weight matrix connecting the hidden layer to the output, $h_1$ is the output from the first hidden layer, and $b_{ho}$ is the bias term.\n",
        "\n",
        "\n",
        "Next, we differentiate $Z_2$ with respect to $h_1$. Since $W_{ho}$ and $b_{ho}$ are independent of $h_1$, we focus on the term $W_{ho}^T h_1$. The derivative of this term with respect to $h_1$ is simply:\n",
        "\n",
        "\n",
        "$\n",
        "\\frac{dZ_2}{dh_1} = \\frac{d}{dh_1}(W_{ho}^T h_1 + b_{ho})\n",
        "$\n",
        "\n",
        "\n",
        "The derivative of $W_{ho}^T h_1$ with respect to $h_1$ is $W_{ho}^T$, and the derivative of the constant $b_{ho}$ is zero. Therefore, we have:\n",
        "\n",
        "\n",
        "$\n",
        "\\frac{dZ_2}{dh_1} = W_{ho}^T\n",
        "$\n",
        "\n",
        "\n",
        "Thus, the partial derivative of $Z_2$ with respect to $h_1$ is equal to the transpose of the weight matrix $W_{ho}$.\n",
        "\n",
        "\n",
        "In summary, we find that:\n",
        "\n",
        "\n",
        "$\n",
        "\\frac{dZ_2}{dh_1} = W_{ho}^T\n",
        "$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dZ_2}{dh_1}$ is equal to the transpose of the weight matrix $W_{ho}$, which will guide the adjustment of the hidden layer's output during the training process.\n",
        "\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To find the dimensions involved in the derivative $\\frac{dZ_2}{dh_1}$, we first note the dimensions of the components involved.\n",
        "\n",
        "\n",
        "The matrix $W_{ho}$ has dimensions $(m_1, 1)$, where $m_1$ is the number of neurons in the hidden layer, and the output $h_1$ from the first layer has dimensions $(m_1, n)$, where $n$ is the number of observations. The bias term $b_{ho}$ typically has dimensions $(1, n)$.\n",
        "\n",
        "\n",
        "The equation for $Z_2$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "Z_2 = W_{ho}^T h_1 + b_{ho}\n",
        "$$\n",
        "\n",
        "\n",
        "As we determined, $\\frac{dZ_2}{dh_1} = W_{ho}^T$. Therefore, the dimension of $\\frac{dZ_2}{dh_1}$ corresponds to the dimension of $W_{ho}^T$.\n",
        "\n",
        "\n",
        "Since $W_{ho}$ has dimensions $(m_1, 1)$, $W_{ho}^T$ has dimensions $(1, m_1)$.\n",
        "\n",
        "\n",
        "Thus, the dimension of $\\frac{dZ_2}{dh_1}$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dZ_2}{dh_1} = (1, m_1)\n",
        "$$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dZ_2}{dh_1}$ has 1 row and $m_1$ columns, corresponding to the transpose of the weight matrix connecting the hidden layer to the output, which is used to adjust the hidden layer's output during the training process.\n"
      ],
      "metadata": {
        "id": "JRiVWC4EndZL"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amuoR7h6NFVF"
      },
      "source": [
        "# rate of change of Z2 w.r.t. hidden layer activations\n",
        "Z2_wrt_h1 = w_ho # we can use w_ho instead of the transpose to ease the comprehension of the following computations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d. Rate of change of hidden layer activations w.r.t Z1**\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find $\\frac{dh_1}{dZ_1}$, we know that $h_1 = \\sigma(Z_1)$. Therefore, the derivative of $h_1$ with respect to $Z_1$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dh_1}{dZ_1} = \\frac{d}{dZ_1} \\sigma(Z_1)\n",
        "$$\n",
        "\n",
        "\n",
        "Since the derivative of the sigmoid function is $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$, we have:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dh_1}{dZ_1} = \\sigma'(Z_1) = \\sigma(Z_1)(1 - \\sigma(Z_1))\n",
        "$$\n",
        "\n",
        "\n",
        "We also know that $h_1 = \\sigma(Z_1)$, so we can rewrite this as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dh_1}{dZ_1} = h_1(1 - h_1)\n",
        "$$\n",
        "\n",
        "\n",
        "Thus, the partial derivative of $h_1$ with respect to $Z_1$ when using the sigmoid activation function is $h_1(1 - h_1)$.\n",
        "\n",
        "\n",
        "In summary, we find that:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dh_1}{dZ_1} = h_1(1 - h_1)\n",
        "$$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dh_1}{dZ_1}$ is equal to the sigmoid function evaluated at $Z_1$ times $(1 - $ the sigmoid function evaluated at $Z_1)$, which will guide the adjustment of $Z_1$ during the training process.\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To determine the dimension of $\\frac{dh_1}{dZ_1}$, we first consider the dimensions of $h_1$ and $Z_1$.\n",
        "\n",
        "\n",
        "Given that $h_1$ is the output of the first layer, it has dimensions $(m_1, n)$, where $m_1$ is the number of neurons in the first layer, and $n$ is the number of observations. Since $Z_1$ is the input to the activation function that produces $h_1$, it must have the same dimensions as $h_1$. Therefore, $Z_1$ also has dimensions $(m_1, n)$.\n",
        "\n",
        "\n",
        "Since $\\frac{dh_1}{dZ_1} = h_1(1 - h_1)$, and both $h_1$ and $Z_1$ have dimensions $(m_1, n)$, the derivative $\\frac{dh_1}{dZ_1}$ will also have the same dimensions. This is because the sigmoid function and its derivative are applied element-wise.\n",
        "\n",
        "\n",
        "Thus, the dimension of $\\frac{dh_1}{dZ_1}$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dh_1}{dZ_1} = (m_1, n)\n",
        "$$\n",
        "\n",
        "\n",
        "This means that the partial derivative $\\frac{dh_1}{dZ_1}$ has $m_1$ rows and $n$ columns, corresponding to the element-wise derivative of the sigmoid function applied to each element of $Z_1$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iz64pH7nf0a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDUZEdWKNFVJ"
      },
      "source": [
        "# rate of change of hidden layer activations w.r.t. Z1\n",
        "h1_wrt_Z1 = np.multiply(hiddenlayer_activations,(1-hiddenlayer_activations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e. Rate of change of Z1 w.r.t weights between input and hidden layer**\n",
        "- `Mathematical notation:`\n",
        "\n",
        "To find the partial derivative $\\frac{dZ_1}{dW_{ih}}$, let's first define the relevant terms.\n",
        "\n",
        "\n",
        "$Z_1$ is the input to the first layer's activation function, and it's given by:\n",
        "\n",
        "\n",
        "$$\n",
        "Z_1 = W_{ih}X + b_{ih}\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "$W_{ih}$ is the weight matrix connecting the input layer to the first hidden layer.\n",
        "$X$ is the input data.\n",
        "$b_{ih}$ is the bias vector for the first hidden layer.\n",
        "\n",
        "\n",
        "Now, we want to find $\\frac{dZ_1}{dW_{ih}}$. Taking the derivative of $Z_1$ with respect to $W_{ih}$, we treat $X$ and $b_{ih}$ as constants since they do not depend on $W_{ih}$. Thus, we have:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_1}{dW_{ih}} = \\frac{d}{dW_{ih}} (W_{ih}X + b_{ih})\n",
        "$$\n",
        "\n",
        "\n",
        "The derivative of $W_{ih}X$ with respect to $W_{ih}$ is simply $X$. The derivative of $b_{ih}$ with respect to $W_{ih}$ is zero, since $b_{ih}$ is independent of $W_{ih}$. Therefore:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_1}{dW_{ih}} = X\n",
        "$$\n",
        "\n",
        "\n",
        "So, the partial derivative of $Z_1$ with respect to $W_{ih}$ is equal to the input data $X$.\n",
        "\n",
        "\n",
        "In summary, we find that:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_1}{dW_{ih}} = X\n",
        "$$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dZ_1}{dW_{ih}}$ is equal to the input data $X$, which is used to adjust the weights $W_{ih}$ during the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- `Dimensions:`\n",
        "\n",
        "To find the dimension of $\\frac{dZ_1}{dW_{ih}}$, let's first define the relevant terms.\n",
        "\n",
        "\n",
        "We know that:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{dZ_1}{dW_{ih}} = X\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "$X$ is the input data.\n",
        "\n",
        "\n",
        "Now, we want to find the dimension of $\\frac{dZ_1}{dW_{ih}}$, which is the same as the dimension of $X$.\n",
        "\n",
        "\n",
        "Let's define the dimensions:\n",
        "$X$ typically has dimensions $(m, n)$, where $m$ is the number of features (variables) and $n$ is the number of observations.\n",
        "\n",
        "\n",
        "Therefore:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dZ_1}{dW_{ih}} = \\text{Dimension of } X = (m, n)\n",
        "$$\n",
        "\n",
        "\n",
        "So, the dimension of $\\frac{dZ_1}{dW_{ih}}$ is:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Dimension of } \\frac{dZ_1}{dW_{ih}} = (m, n)\n",
        "$$\n",
        "\n",
        "\n",
        "This result indicates that the partial derivative $\\frac{dZ_1}{dW_{ih}}$ has dimensions $m \\times n$, matching the dimensions of the input data $X$, where $m$ is the number of features and $n$ is the number of observations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wrs7JjpYnjqt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft4U6Td6NFVO"
      },
      "source": [
        "# rate of change of Z1 w.r.t. weights between input and hidden layer\n",
        "Z1_wrt_wih = X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-hsfsi4NFVR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3b0d0b-dd41-4151-aff0-d2f03f57c362"
      },
      "source": [
        "# checking the shapes of partial derivatives\n",
        "error_wrt_output.shape, output_wrt_Z2.shape, Z2_wrt_h1.shape, h1_wrt_Z1.shape, Z1_wrt_wih.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 8), (1, 8), (3, 1), (3, 8), (5, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uka_yPrNFVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52c7510-c2a0-42a7-9eaa-66ed746b5fb6"
      },
      "source": [
        "# shape of weights of hidden layer\n",
        "w_ih.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCeSm7vrHbHj"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1RkG5x1NEFWlF3tj0OlswOWvBcV5XNV1C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTPNf3E5NFVs"
      },
      "source": [
        "# rate of change of error w.r.t weights between input and hidden layer\n",
        "error_wrt_wih = np.dot(Z1_wrt_wih,(h1_wrt_Z1*np.dot(Z2_wrt_h1,(output_wrt_Z2*error_wrt_output))).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_wrt_wih"
      ],
      "metadata": {
        "id": "-qTQO_WOH5LW",
        "outputId": "62033289-5373-42fa-c2df-e249d900d077",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0100051 , -0.00455386, -0.00235828],\n",
              "       [ 0.00761805,  0.00348603,  0.00540206],\n",
              "       [-0.00921193, -0.00438709, -0.0036097 ],\n",
              "       [ 0.00542114,  0.0032178 ,  0.00404109],\n",
              "       [-0.01918392, -0.00745773, -0.00338612]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WN0I-mpNFVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64238e84-9c37-43d9-9d8d-8fbd3e870ba8"
      },
      "source": [
        "error_wrt_wih.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2bu4H5-NFVz"
      },
      "source": [
        "### Update the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nmJnY_PNFV1"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1A5jaB3WjZx9yrJkk9imVEvP3PZodjapE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r59xEpINFV2"
      },
      "source": [
        "# defining the learning rate\n",
        "lr = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiBFNXd3NFV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ec2e39-b232-4993-a49b-f2b16c123fa8"
      },
      "source": [
        "# initial w_ho and w_ih\n",
        "w_ho"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8952718 ],\n",
              "       [0.26443251],\n",
              "       [0.2374307 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuosFKUENFWB",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d531c6-1c58-475f-8b67-1f7f424c11ef"
      },
      "source": [
        "w_ih"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87870937, 0.66846918, 0.30019003],\n",
              "       [0.89796433, 0.92627396, 0.77923104],\n",
              "       [0.664842  , 0.50687411, 0.27885069],\n",
              "       [0.55219686, 0.79078026, 0.12192073],\n",
              "       [0.5606317 , 0.00277591, 0.72874415]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_Va2xywNFWF"
      },
      "source": [
        "# updating the weights of output layer\n",
        "w_ho = w_ho - lr * error_wrt_who"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruFlc96BNFWL"
      },
      "source": [
        "# updating the weights of hidden layer\n",
        "w_ih = w_ih - lr * error_wrt_wih"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTf4nS1xNFWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3a1b3f-7136-4c19-fb88-fc95bd3f0fb6"
      },
      "source": [
        "# updated w_ho and w_ih\n",
        "w_ho"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8949962 ],\n",
              "       [0.2639417 ],\n",
              "       [0.23713201]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VYNPPNlNFWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6dfcdd0-d0d4-41e1-f803-5d6e82d1f9ad"
      },
      "source": [
        "w_ih"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87880943, 0.66851472, 0.30021361],\n",
              "       [0.89788815, 0.9262391 , 0.77917702],\n",
              "       [0.66493412, 0.50691798, 0.27888679],\n",
              "       [0.55214265, 0.79074809, 0.12188032],\n",
              "       [0.56082353, 0.00285049, 0.72877801]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxLy6DZlNFWY"
      },
      "source": [
        "## 6. Training the model for n epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HKS9vIyNFWZ"
      },
      "source": [
        "# defining the model architecture\n",
        "inputlayer_neurons = X.shape[0] # number of features in data set\n",
        "hiddenlayer_neurons = 3 # number of hidden layers neurons\n",
        "output_neurons = 1 # number of neurons at output layer\n",
        "\n",
        "# initializing weight\n",
        "w_ih=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "w_ho=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "\n",
        "# defining the parameters (hyperparameters)\n",
        "lr = 0.01\n",
        "epochs = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojybM51LNFWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442d7f32-6cf6-4a34-b738-1b264d4ece52"
      },
      "source": [
        "# initial w_ih and w_ho\n",
        "w_ih"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39692748, 0.0940208 , 0.75301087],\n",
              "       [0.13666712, 0.76544151, 0.61367941],\n",
              "       [0.43845803, 0.69074318, 0.93420727],\n",
              "       [0.43628423, 0.56397198, 0.17795248],\n",
              "       [0.36115595, 0.09581573, 0.07160845]])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RS_d3kdNFWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22eeab41-15b3-4b75-d657-f9f00c9a2690"
      },
      "source": [
        "w_ho"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91876566],\n",
              "       [0.8091531 ],\n",
              "       [0.11724062]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yVAcyW_NFWk"
      },
      "source": [
        "error_epoch = []\n",
        "for i in range(epochs):\n",
        "    # Forward Propogation\n",
        "\n",
        "    # hidden layer activations\n",
        "    hidden_layer_input=np.dot(w_ih.T,X)\n",
        "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
        "    # calculating the output\n",
        "    output_layer_input=np.dot(w_ho.T,hiddenlayer_activations)\n",
        "    output = sigmoid(output_layer_input)\n",
        "\n",
        "    #===================================================================\n",
        "    # Backward Propagation\n",
        "\n",
        "    # calculating error\n",
        "    error = np.square(y-output)/2\n",
        "    error_wrt_output = -(y-output)\n",
        "    output_wrt_Z2 = np.multiply(output,(1-output))\n",
        "    Z2_wrt_who = hiddenlayer_activations\n",
        "    # rate of change of error w.r.t weight between hidden and output layer\n",
        "    error_wrt_who = np.dot(Z2_wrt_who,(error_wrt_output*output_wrt_Z2).T)\n",
        "    Z2_wrt_h1 = w_ho\n",
        "    h1_wrt_Z1 = np.multiply(hiddenlayer_activations,(1-hiddenlayer_activations))\n",
        "    Z1_wrt_wih = X\n",
        "    # rate of change of error w.r.t weights between input and hidden layer\n",
        "    error_wrt_wih = np.dot(Z1_wrt_wih,(h1_wrt_Z1*np.dot(Z2_wrt_h1,(error_wrt_output*output_wrt_Z2))).T)\n",
        "\n",
        "    #===================================================================\n",
        "\n",
        "    # updating the weights between hidden and output layer\n",
        "    w_ho = w_ho - lr * error_wrt_who\n",
        "    # updating the weights between input and hidden layer\n",
        "    w_ih = w_ih - lr * error_wrt_wih\n",
        "\n",
        "    # appending the error of each epoch\n",
        "    error_epoch.append(np.average(error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra5mTgwUNFWo",
        "outputId": "b8d62a57-bf39-4535-e18b-f76775ba14b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# updated w_ih and w_ho\n",
        "w_ih"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.93592932,  0.16350934,  0.4808718 ],\n",
              "       [-3.86329771,  0.75144858,  1.30370497],\n",
              "       [ 2.7337959 ,  0.6924831 ,  0.34051738],\n",
              "       [-3.37424933,  0.47785603,  0.69862832],\n",
              "       [ 0.66190167,  0.3023947 , -0.30203652]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeN2dcc0NFW8",
        "scrolled": true,
        "outputId": "b1c1c7a4-3554-4a35-ee46-14130f0b4c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "# visualizing the error after each epoch\n",
        "plt.plot(np.arange(1,epochs+1), np.array(error_epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7244643010>]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`Comments:`** The loss is decreasing very fast after 5000 epochs of training and begins to stabilize at 10000 epochs. The final loss is close to 0. $E  0$"
      ],
      "metadata": {
        "id": "SjBOThw0jj1v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlcGoeUNFXA",
        "outputId": "d87bdc3c-9f6f-4686-d19c-0a1aac298598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# final output from the model\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99345447, 0.94881699, 0.03822425, 0.99345447, 0.94914295,\n",
              "        0.07757236, 0.99281065, 0.96129248]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARNn3MiKNFXF",
        "outputId": "153cc300-8b5f-413b-84a7-a9df00f7d243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# actual target\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 1, 1, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtwXl0bjNFXJ",
        "outputId": "62a2ddd9-f194-42d8-fdea-78b5c2f75287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# error at last epoch\n",
        "np.mean(error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0008950218790843358"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}